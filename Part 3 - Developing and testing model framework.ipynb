{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NOTE\n### CHeck this for basic outline of building a NN in pytorch - \n### https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n\n### https://machinelearningmastery.com/building-a-multiclass-classification-model-in-pytorch/\n\n### https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html\n","metadata":{}},{"cell_type":"markdown","source":"# TODO\n\n## 1. Add stop criteria for epoch. Training is working perfect\n## 1. Integrate fusion framework with NN\n## 1. Add full datasets. GPU trains super fast\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages `to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-12T04:48:14.454320Z","iopub.execute_input":"2023-03-12T04:48:14.454978Z","iopub.status.idle":"2023-03-12T04:48:16.834772Z","shell.execute_reply.started":"2023-03-12T04:48:14.454895Z","shell.execute_reply":"2023-03-12T04:48:16.833706Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/sarcasm/bert_embeddings_768.parquet\n/kaggle/input/sarcasm/non_sarcasm_tweets.csv\n/kaggle/input/sarcasm/sarcasm_tweets.csv\n/kaggle/input/sarcasm/clean_raw_data.parquet\n/kaggle/input/artkdata-private/clean_data_p2_w_bert.parquet\n/kaggle/input/artkdata-private/ATRK dataset.csv\n/kaggle/input/artkdata-private/clean_raw_data.parquet\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"# Set random seed\n\nSEED = 1234509876\n\n# Importing basic libraries\nfrom zipfile import ZipFile\nimport os, sys\nimport re\nimport gc\nimport time\nimport datetime\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom string import punctuation\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\n%matplotlib inline\n\n# Import huggingface library\nimport transformers\n\n# Import NLTK\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nfrom wordcloud import WordCloud\n# Import models\n\nimport lightgbm as lgb\nfrom sklearn.cluster import KMeans\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# Model selection\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Others\n\nfrom tqdm import tqdm_notebook #Loads progressbars for various loops\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n#####################\n#Useful pandas settings\n\npd.set_option('display.max_rows', 400)\npd.set_option('display.max_columns', 160)\npd.set_option('display.max_colwidth', 40)\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-03-12T04:48:16.836541Z","iopub.execute_input":"2023-03-12T04:48:16.837692Z","iopub.status.idle":"2023-03-12T04:48:20.691909Z","shell.execute_reply.started":"2023-03-12T04:48:16.837655Z","shell.execute_reply":"2023-03-12T04:48:20.689697Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"# Import pytorch stuff","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# Check and assign a GPU instance\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-03-12T04:48:20.695391Z","iopub.execute_input":"2023-03-12T04:48:20.696322Z","iopub.status.idle":"2023-03-12T04:48:20.704826Z","shell.execute_reply.started":"2023-03-12T04:48:20.696270Z","shell.execute_reply":"2023-03-12T04:48:20.703891Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Import dataset","metadata":{}},{"cell_type":"code","source":"# IMPORT EXPORT CELL FOR PARQUET\n\n##########################################\n# IMPORT\nraw_data = pq.read_table('/kaggle/input/artkdata-private/clean_data_p2_w_bert.parquet').to_pandas()\n\n# ##########################################\n# # EXPORT\n# temp = pd.DataFrame(bert_word_embeddings)\n# temp_col_names = temp.columns.values\n# temp_col_names = [\"bert_emb_\"+str(col) for col in temp_col_names]\n\n\n# temp.columns = temp_col_names\n\n# temp.to_parquet(path = '/kaggle/working/bert_embeddings.parquet')","metadata":{"execution":{"iopub.status.busy":"2023-03-12T04:48:20.710312Z","iopub.execute_input":"2023-03-12T04:48:20.711878Z","iopub.status.idle":"2023-03-12T04:48:23.658845Z","shell.execute_reply.started":"2023-03-12T04:48:20.711841Z","shell.execute_reply":"2023-03-12T04:48:23.657810Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"raw_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-12T04:48:23.660298Z","iopub.execute_input":"2023-03-12T04:48:23.660682Z","iopub.status.idle":"2023-03-12T04:48:23.686371Z","shell.execute_reply.started":"2023-03-12T04:48:23.660646Z","shell.execute_reply":"2023-03-12T04:48:23.685522Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                      text   labels  \\\n0  Youll notice on the new episode comi...  sarcasm   \n1  @politicalHEDGE @BernieSanders Well ...  sarcasm   \n2  @ajodom60 @DeniseAshbaugh @nenanatio...  sarcasm   \n3  #livestreaming #Modeling Merlin for ...  sarcasm   \n4  You spin me right round! Wholesale? ...  sarcasm   \n\n                                clean_text emoji_list  \\\n0  Youll notice on the new episode comi...         []   \n1  SOME_ENTITY SOME_ENTITY Well arent y...         []   \n2  SOME_ENTITY SOME_ENTITY SOME_ENTITY ...         []   \n3  Merlin for the . and if you like , a...         []   \n4  You spin me right round wholesale I ...         []   \n\n                            clean_text_lem  \\\n0  Youll notice on the new episode comi...   \n1  SOME_ENTITY SOME_ENTITY Well arent y...   \n2  SOME_ENTITY SOME_ENTITY SOME_ENTITY ...   \n3  Merlin for the . and if you like , a...   \n4  You spin me right round wholesale I ...   \n\n                       clean_text_lem_stop  \\\n0  Youll notice new episode coming week...   \n1  SOME_ENTITY SOME_ENTITY Well arent l...   \n2  SOME_ENTITY SOME_ENTITY SOME_ENTITY ...   \n3                        Merlin . like , .   \n4  You spin right round wholesale I Hav...   \n\n                              bert_emb_768  \n0  [-0.191491037607193, 0.0230156742036...  \n1  [0.21377572417259216, 0.089411765336...  \n2  [0.1151486411690712, 0.0453656576573...  \n3  [0.109009750187397, -0.2241743505001...  \n4  [0.7060186266899109, 0.0676050409674...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>labels</th>\n      <th>clean_text</th>\n      <th>emoji_list</th>\n      <th>clean_text_lem</th>\n      <th>clean_text_lem_stop</th>\n      <th>bert_emb_768</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Youll notice on the new episode comi...</td>\n      <td>sarcasm</td>\n      <td>Youll notice on the new episode comi...</td>\n      <td>[]</td>\n      <td>Youll notice on the new episode comi...</td>\n      <td>Youll notice new episode coming week...</td>\n      <td>[-0.191491037607193, 0.0230156742036...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@politicalHEDGE @BernieSanders Well ...</td>\n      <td>sarcasm</td>\n      <td>SOME_ENTITY SOME_ENTITY Well arent y...</td>\n      <td>[]</td>\n      <td>SOME_ENTITY SOME_ENTITY Well arent y...</td>\n      <td>SOME_ENTITY SOME_ENTITY Well arent l...</td>\n      <td>[0.21377572417259216, 0.089411765336...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@ajodom60 @DeniseAshbaugh @nenanatio...</td>\n      <td>sarcasm</td>\n      <td>SOME_ENTITY SOME_ENTITY SOME_ENTITY ...</td>\n      <td>[]</td>\n      <td>SOME_ENTITY SOME_ENTITY SOME_ENTITY ...</td>\n      <td>SOME_ENTITY SOME_ENTITY SOME_ENTITY ...</td>\n      <td>[0.1151486411690712, 0.0453656576573...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#livestreaming #Modeling Merlin for ...</td>\n      <td>sarcasm</td>\n      <td>Merlin for the . and if you like , a...</td>\n      <td>[]</td>\n      <td>Merlin for the . and if you like , a...</td>\n      <td>Merlin . like , .</td>\n      <td>[0.109009750187397, -0.2241743505001...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>You spin me right round! Wholesale? ...</td>\n      <td>sarcasm</td>\n      <td>You spin me right round wholesale I ...</td>\n      <td>[]</td>\n      <td>You spin me right round wholesale I ...</td>\n      <td>You spin right round wholesale I Hav...</td>\n      <td>[0.7060186266899109, 0.0676050409674...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Temporary features","metadata":{}},{"cell_type":"code","source":"raw_data['text_len'] = raw_data['clean_text'].apply(lambda x: len(x.split(\" \")))\nraw_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-12T04:48:23.688027Z","iopub.execute_input":"2023-03-12T04:48:23.688922Z","iopub.status.idle":"2023-03-12T04:48:23.745262Z","shell.execute_reply.started":"2023-03-12T04:48:23.688885Z","shell.execute_reply":"2023-03-12T04:48:23.744223Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                      text   labels  \\\n0  Youll notice on the new episode comi...  sarcasm   \n1  @politicalHEDGE @BernieSanders Well ...  sarcasm   \n2  @ajodom60 @DeniseAshbaugh @nenanatio...  sarcasm   \n3  #livestreaming #Modeling Merlin for ...  sarcasm   \n4  You spin me right round! Wholesale? ...  sarcasm   \n\n                                clean_text emoji_list  \\\n0  Youll notice on the new episode comi...         []   \n1  SOME_ENTITY SOME_ENTITY Well arent y...         []   \n2  SOME_ENTITY SOME_ENTITY SOME_ENTITY ...         []   \n3  Merlin for the . and if you like , a...         []   \n4  You spin me right round wholesale I ...         []   \n\n                            clean_text_lem  \\\n0  Youll notice on the new episode comi...   \n1  SOME_ENTITY SOME_ENTITY Well arent y...   \n2  SOME_ENTITY SOME_ENTITY SOME_ENTITY ...   \n3  Merlin for the . and if you like , a...   \n4  You spin me right round wholesale I ...   \n\n                       clean_text_lem_stop  \\\n0  Youll notice new episode coming week...   \n1  SOME_ENTITY SOME_ENTITY Well arent l...   \n2  SOME_ENTITY SOME_ENTITY SOME_ENTITY ...   \n3                        Merlin . like , .   \n4  You spin right round wholesale I Hav...   \n\n                              bert_emb_768  text_len  \n0  [-0.191491037607193, 0.0230156742036...        35  \n1  [0.21377572417259216, 0.089411765336...        21  \n2  [0.1151486411690712, 0.0453656576573...        23  \n3  [0.109009750187397, -0.2241743505001...        11  \n4  [0.7060186266899109, 0.0676050409674...        31  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>labels</th>\n      <th>clean_text</th>\n      <th>emoji_list</th>\n      <th>clean_text_lem</th>\n      <th>clean_text_lem_stop</th>\n      <th>bert_emb_768</th>\n      <th>text_len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Youll notice on the new episode comi...</td>\n      <td>sarcasm</td>\n      <td>Youll notice on the new episode comi...</td>\n      <td>[]</td>\n      <td>Youll notice on the new episode comi...</td>\n      <td>Youll notice new episode coming week...</td>\n      <td>[-0.191491037607193, 0.0230156742036...</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@politicalHEDGE @BernieSanders Well ...</td>\n      <td>sarcasm</td>\n      <td>SOME_ENTITY SOME_ENTITY Well arent y...</td>\n      <td>[]</td>\n      <td>SOME_ENTITY SOME_ENTITY Well arent y...</td>\n      <td>SOME_ENTITY SOME_ENTITY Well arent l...</td>\n      <td>[0.21377572417259216, 0.089411765336...</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@ajodom60 @DeniseAshbaugh @nenanatio...</td>\n      <td>sarcasm</td>\n      <td>SOME_ENTITY SOME_ENTITY SOME_ENTITY ...</td>\n      <td>[]</td>\n      <td>SOME_ENTITY SOME_ENTITY SOME_ENTITY ...</td>\n      <td>SOME_ENTITY SOME_ENTITY SOME_ENTITY ...</td>\n      <td>[0.1151486411690712, 0.0453656576573...</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#livestreaming #Modeling Merlin for ...</td>\n      <td>sarcasm</td>\n      <td>Merlin for the . and if you like , a...</td>\n      <td>[]</td>\n      <td>Merlin for the . and if you like , a...</td>\n      <td>Merlin . like , .</td>\n      <td>[0.109009750187397, -0.2241743505001...</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>You spin me right round! Wholesale? ...</td>\n      <td>sarcasm</td>\n      <td>You spin me right round wholesale I ...</td>\n      <td>[]</td>\n      <td>You spin me right round wholesale I ...</td>\n      <td>You spin right round wholesale I Hav...</td>\n      <td>[0.7060186266899109, 0.0676050409674...</td>\n      <td>31</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Remove entries that did not process properly\n\nBERT embeddings did not process for all entries. Dropping invalid entries (n=90) to avoid issues.","metadata":{}},{"cell_type":"code","source":"raw_data.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-12T04:48:23.746746Z","iopub.execute_input":"2023-03-12T04:48:23.747171Z","iopub.status.idle":"2023-03-12T04:48:23.754393Z","shell.execute_reply.started":"2023-03-12T04:48:23.747135Z","shell.execute_reply":"2023-03-12T04:48:23.753402Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(30000, 8)"},"metadata":{}}]},{"cell_type":"code","source":"raw_data = raw_data.loc[~raw_data['bert_emb_768'].isna(),:]\n\nraw_data.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-12T04:48:23.755705Z","iopub.execute_input":"2023-03-12T04:48:23.756709Z","iopub.status.idle":"2023-03-12T04:48:23.771347Z","shell.execute_reply.started":"2023-03-12T04:48:23.756673Z","shell.execute_reply":"2023-03-12T04:48:23.770163Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(29910, 8)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Split dataset and assign the data loader","metadata":{}},{"cell_type":"code","source":"target_labels = raw_data.loc[:,'labels']\n\ntarget_labels = (target_labels == \"sarcasm\").astype(int)\ntarget_labels","metadata":{"execution":{"iopub.status.busy":"2023-03-12T04:48:23.772736Z","iopub.execute_input":"2023-03-12T04:48:23.773140Z","iopub.status.idle":"2023-03-12T04:48:23.785797Z","shell.execute_reply.started":"2023-03-12T04:48:23.773106Z","shell.execute_reply":"2023-03-12T04:48:23.784744Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"0        1\n1        1\n2        1\n3        1\n4        1\n        ..\n29995    0\n29996    0\n29997    0\n29998    0\n29999    0\nName: labels, Length: 29910, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_data, test_data = train_test_split(raw_data, train_size=0.8, stratify=raw_data['labels'])","metadata":{"execution":{"iopub.status.busy":"2023-03-12T04:48:23.790412Z","iopub.execute_input":"2023-03-12T04:48:23.790687Z","iopub.status.idle":"2023-03-12T04:48:23.824050Z","shell.execute_reply.started":"2023-03-12T04:48:23.790652Z","shell.execute_reply":"2023-03-12T04:48:23.823207Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Check if stratified samples are good\n\ntrain_data.groupby('labels').count()['text'],test_data.groupby('labels').count()['text'] ","metadata":{"execution":{"iopub.status.busy":"2023-03-12T04:48:23.825303Z","iopub.execute_input":"2023-03-12T04:48:23.825623Z","iopub.status.idle":"2023-03-12T04:48:23.859354Z","shell.execute_reply.started":"2023-03-12T04:48:23.825591Z","shell.execute_reply":"2023-03-12T04:48:23.858292Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(labels\n non-sarcasm    12789\n sarcasm        11139\n Name: text, dtype: int64,\n labels\n non-sarcasm    3197\n sarcasm        2785\n Name: text, dtype: int64)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Make dataloader to manage input\n\n1. Extract required columns and assign them to a proper dataloader\n1. Tweak dataloader if new features are added\n\nLink - https://stackoverflow.com/questions/51444059/how-to-iterate-over-two-dataloaders-simultaneously-using-pytorch","metadata":{}},{"cell_type":"code","source":"# Quick function to auto assign required coluimns and make a tensor dataset\n# class CustomTensorDataset(Dataset):\n#     def __init__(self, dataset):\n#         super(CustomTensorDataset, self).__init__()\n        \n#         self.dataset = dataset\n#         self.target = dataset[0] # Targets will be stored first\n#         self.base_embeddings = dataset[1] # Basic embeddings will be index 1\n#         self.bert_emb_cols = dataset[2] # BERT embeddings will be index 2\n#         self.add_feat_cols = dataset[3] # additional features will be index 3\n    \n#     def __len__(self):\n#         return len(self.__len__)\n\n#     def __getitem__(self, idx):\n#         if torch.is_tensor(idx):\n#             idx = idx.tolist()\n\n#         img_name = os.path.join(self.root_dir,\n#                                 self.landmarks_frame.iloc[idx, 0])\n#         image = io.imread(img_name)\n#         landmarks = self.landmarks_frame.iloc[idx, 1:]\n#         landmarks = np.array([landmarks])\n#         landmarks = landmarks.astype('float').reshape(-1, 2)\n#         sample = {'image': image, 'landmarks': landmarks}\n\n#         if self.transform:\n#             sample = self.transform(sample)\n\n#         return sample\n    \n        \ndef usable_numpy_mat(df, base_emb_cols, bert_emb_cols, add_feat_cols, target):\n    # Take a dataframe and given column haeaders and convert it to simple to use pytorch dataset\n    # Will retain order of BERT EMBS >>> BASE EMBS/BASE EMB PREDS >>> ADD FEATUIRES\n    # Returns 2 arrays, 1 with all features, 1 with targets\n    \n    # NOT NEEDED ANYMORE\n    # lambda func for converting embeddings stored in a single column to numpy array\n    # emb_column_to_numpy = lambda array: np.vstack(a[0] for a in array.values) \n    \n    # One column will have numpy arrays with embeddings\n    base_embs = np.vstack(df.loc[:,base_emb_cols].values)\n    bert_embs = np.vstack(df.loc[:,bert_emb_cols].values)\n    \n    # One column per feature present\n    add_feats = df.loc[:,add_feat_cols].to_numpy()\n    \n    targets = (df.loc[:,'labels'] == 'sarcasm').astype(int).to_numpy()\n    \n    # Merge all features into one numpy frame\n    all_feats = np.column_stack((bert_embs, base_embs, add_feats))\n    \n    return all_feats, targets\n    ","metadata":{"execution":{"iopub.status.busy":"2023-03-12T04:48:23.860995Z","iopub.execute_input":"2023-03-12T04:48:23.861356Z","iopub.status.idle":"2023-03-12T04:48:23.868914Z","shell.execute_reply.started":"2023-03-12T04:48:23.861321Z","shell.execute_reply":"2023-03-12T04:48:23.867823Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"\n\ntrain_feat_arr, train_tgt_arr = usable_numpy_mat(df = train_data,\n                                    base_emb_cols = ('bert_emb_768'),\n                                    bert_emb_cols = ('bert_emb_768'),\n                                    add_feat_cols = ('text_len'), \n                                    target = ('labels'))\n\n\ntest_feat_arr, test_tgt_arr = usable_numpy_mat(df = test_data,\n                                    base_emb_cols = ('bert_emb_768'),\n                                    bert_emb_cols = ('bert_emb_768'),\n                                    add_feat_cols = ('text_len'), \n                                    target = ('labels'))\n\n###\n#REF\n# targets = torch.from_numpy((df.loc[:,'labels'] == 'sarcasm').astype(int).to_numpy()).to(device)\n    \ntrain_dataset_tensor = torch.utils.data.TensorDataset(torch.from_numpy(train_feat_arr),torch.from_numpy(train_tgt_arr))\ntest_dataset_tensor = torch.utils.data.TensorDataset(torch.from_numpy(test_feat_arr),torch.from_numpy(test_tgt_arr))","metadata":{"execution":{"iopub.status.busy":"2023-03-12T04:48:23.870137Z","iopub.execute_input":"2023-03-12T04:48:23.871105Z","iopub.status.idle":"2023-03-12T04:48:24.207957Z","shell.execute_reply.started":"2023-03-12T04:48:23.871078Z","shell.execute_reply":"2023-03-12T04:48:24.206971Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_data_loader = torch.utils.data.DataLoader(dataset = train_dataset_tensor, batch_size = 128, shuffle= True)\ntest_data_loader = torch.utils.data.DataLoader(dataset = test_dataset_tensor, batch_size = 128, shuffle= True)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T04:48:24.209276Z","iopub.execute_input":"2023-03-12T04:48:24.209958Z","iopub.status.idle":"2023-03-12T04:48:24.216277Z","shell.execute_reply.started":"2023-03-12T04:48:24.209920Z","shell.execute_reply":"2023-03-12T04:48:24.215002Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Make the BNeural Network","metadata":{}},{"cell_type":"code","source":"# Base model will use basic word embeddings to supply a prediction.\n\ndef multi_framework_classifier(base_data_in, fused_data_in, target_labels, base_model_in, fused_model_in):\n    # WIP\n    # Base data is only expected to have word embeddings for the base model\n    # The fused data will have features + embeddings\n    # Labels will be common for both datasets, so ensure order does not change\n    \n    ##################################################\n    # Copy of inputs to avoid replacing anything\n    base_model = base_model_in\n    fused_model = fused_model_in\n    \n    base_data = base_data_in\n    fused_data = fused_data_in\n    target_labels = target_labels_in\n    \n    # Fit each model on data provided\n    base_pred = base_model.fit(base_data, target_labels)\n    fused_pred = fused_model.fit(fused_data, target_labels)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-12T04:48:24.217912Z","iopub.execute_input":"2023-03-12T04:48:24.218342Z","iopub.status.idle":"2023-03-12T04:48:24.229912Z","shell.execute_reply.started":"2023-03-12T04:48:24.218306Z","shell.execute_reply":"2023-03-12T04:48:24.229012Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class NeuralNetwork(nn.Module):\n    def __init__(self, n_input_feats):\n        # Defualt initiation fo Pytorch NN\n        super(NeuralNetwork,self).__init__()\n        \n        # dEFINE DROPOUT FOR THE WHOLE NN\n        self.drop = nn.Dropout(p = 0.3)\n        self.input_layer_size = n_input_feats\n        \n        # Not sure what this does\n        self.flatten = nn.Flatten()\n        \n        # Build sequential stack NN off below layers\n        # 2 Hidden layer\n        print(f\"HEAR YE HEAR YE: Features = {self.input_layer_size} \")\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(in_features = self.input_layer_size, out_features = 256),\n            nn.ReLU(), # ReLU asctivation to replace sigmoid\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, 2),\n        )\n        self.out = nn.Softmax(dim = 1) # SOFT MAX ON EACH ROW\n\n    def forward(self, x):\n        x = self.flatten(x) # Does not matter since we have word embeddings, but keeping it in\n        logits = self.linear_relu_stack(x.float())\n        probs = self.out(logits) #Convert to probablities\n        \n        return probs\n    \n\n# class Classifier_NN(nn.Module):\n    \n#     def __init__(self, n_classes):\n#         # Not sure what this does exactlty.\n#         # Best guess is we inherit the properties from torch and initiate it\n#         super(Classifier_NN, self).__init__()\n        \n#         # Define dropout probablity for NN\n#         self.drop = nn.Dropout(p = 0.3)\n        \n#         # Define output layer\n#         # Hardcoded input size. Change LATER\n#         # Output classes will be just 2\n#         self.out = nn.Linear(in_features = 768, out_features= n_classes)\n        \n#         # We will use softmax for output\n#         self.softmax = nn.Softmax(dim = 1)\n    \n#     def forward(self, ):\n#         output = self.drop(embedding) # Apply dropout\n#         output = self.out(output.float()) # Run input through network\n        \n#         return self.softmax(output) # Return softmax converted output\n        ","metadata":{"execution":{"iopub.status.busy":"2023-03-12T04:48:24.231508Z","iopub.execute_input":"2023-03-12T04:48:24.231890Z","iopub.status.idle":"2023-03-12T04:48:24.243463Z","shell.execute_reply.started":"2023-03-12T04:48:24.231856Z","shell.execute_reply":"2023-03-12T04:48:24.242493Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Make a training function","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n    # Make model into training mode\n    model = model.train()\n    \n    # Save losses for each batch and number of correct predctions\n    losses = []\n    correct_preds = 0\n    \n    for d in data_loader:\n        # iterate using a data loader\n        # Save dataset portions into variables\n        \n        all_feats = d[0].to(device)      \n        targets = d[1].to(device)\n        \n        #Generate predictions\n        outputs = model(all_feats)\n        \n        # Generate predictions\n        _, preds = torch.max(outputs, dim = 1)\n        # Store losses\n        loss = loss_fn(outputs, targets)\n        \n        # Storing current loop info for tracking\n        correct_preds += torch.sum(preds == targets)\n        losses.append(loss.item())\n        \n        # Backprop in model\n        loss.backward()\n        # Compensate for exploding gradients\n        # Using the version of function that works inplace\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)\n        # Step in optimizer and scheduler\n        optimizer.step()\n        scheduler.step()\n        # Unsure what this does\n        optimizer.zero_grad()\n    \n    # Return accuracy and mean of all losses\n    return correct_preds.double()/n_examples, np.mean(losses)\n        ","metadata":{"execution":{"iopub.status.busy":"2023-03-12T04:48:24.244897Z","iopub.execute_input":"2023-03-12T04:48:24.245640Z","iopub.status.idle":"2023-03-12T04:48:24.256822Z","shell.execute_reply.started":"2023-03-12T04:48:24.245597Z","shell.execute_reply":"2023-03-12T04:48:24.256066Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Make evaluation function for model\n\nThis is copy of training function without the steppers","metadata":{}},{"cell_type":"code","source":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n    \n    model = model.eval() # Switch to eval mode so no gradient update\n    \n    losses = []\n    correct_preds = 0\n    with torch.no_grad():\n        for d in data_loader:\n            # iterate using a data loader\n            # Save dataset portions into variables\n\n            all_feats = d[0].to(device)      \n            targets = d[1].to(device)\n\n            #Generate predictions\n            outputs = model(all_feats)\n\n            # Generate predictions\n            _, preds = torch.max(outputs, dim = 1)\n            # Store losses\n            loss = loss_fn(outputs, targets)\n\n            # Storing current loop info for tracking\n            correct_preds += torch.sum(preds == targets)\n            losses.append(loss.item())\n","metadata":{"execution":{"iopub.status.busy":"2023-03-12T04:48:24.257968Z","iopub.execute_input":"2023-03-12T04:48:24.259699Z","iopub.status.idle":"2023-03-12T04:48:24.271232Z","shell.execute_reply.started":"2023-03-12T04:48:24.259657Z","shell.execute_reply":"2023-03-12T04:48:24.270285Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"train_feat_arr.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-03-12T04:48:24.272993Z","iopub.execute_input":"2023-03-12T04:48:24.273398Z","iopub.status.idle":"2023-03-12T04:48:24.285876Z","shell.execute_reply.started":"2023-03-12T04:48:24.273362Z","shell.execute_reply":"2023-03-12T04:48:24.284746Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"23928"},"metadata":{}}]},{"cell_type":"code","source":"EPOCHS = 200\n\nn_train_samples = train_feat_arr.shape[0]\nn_input_feats = train_feat_arr.shape[1]\n\nnn_model = NeuralNetwork(n_input_feats) # Define a new neural network based on previous function\nnn_model = nn_model.to(device) # Attach network to GPU\n\n# Define optiizer for trainign NN\noptimizer = transformers.AdamW(nn_model.parameters(), lr = 2e-5, correct_bias = False)\n\n# Define total number of steps\ntotal_train_steps = n_train_samples * EPOCHS # Number of epochs\n\n# Define a scheduler\nscheduler = transformers.get_linear_schedule_with_warmup(\n    optimizer\n    ,num_warmup_steps= 0\n    ,num_training_steps= total_train_steps\n)\n\nloss_fn = nn.CrossEntropyLoss().to(device)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T04:48:24.287803Z","iopub.execute_input":"2023-03-12T04:48:24.288202Z","iopub.status.idle":"2023-03-12T04:48:33.388064Z","shell.execute_reply.started":"2023-03-12T04:48:24.288168Z","shell.execute_reply":"2023-03-12T04:48:33.386959Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"HEAR YE HEAR YE: Features = 1537 \n","output_type":"stream"}]},{"cell_type":"code","source":"from collections import defaultdict\n\nold_acc, old_loss\n\nfor epoch in range(EPOCHS):\n    \n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    print('-' * 10)\n    \n    train_acc, train_loss = train_epoch(nn_model, train_data_loader, loss_fn, optimizer, device, scheduler,n_train_samples)\n    \n    print(f'Accuracy:{round(float(train_acc),5)} | Loss:{round(float(train_loss),5)}')","metadata":{"execution":{"iopub.status.busy":"2023-03-12T04:48:33.389503Z","iopub.execute_input":"2023-03-12T04:48:33.390379Z","iopub.status.idle":"2023-03-12T04:50:46.270092Z","shell.execute_reply.started":"2023-03-12T04:48:33.390337Z","shell.execute_reply":"2023-03-12T04:50:46.268423Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Epoch 1/2000\n----------\nAccuracy:0.77808 | Loss:0.54459\nEpoch 2/2000\n----------\nAccuracy:0.82092 | Loss:0.48864\nEpoch 3/2000\n----------\nAccuracy:0.82945 | Loss:0.48018\nEpoch 4/2000\n----------\nAccuracy:0.83375 | Loss:0.47536\nEpoch 5/2000\n----------\nAccuracy:0.83697 | Loss:0.47181\nEpoch 6/2000\n----------\nAccuracy:0.84069 | Loss:0.46917\nEpoch 7/2000\n----------\nAccuracy:0.84274 | Loss:0.46673\nEpoch 8/2000\n----------\nAccuracy:0.84504 | Loss:0.46466\nEpoch 9/2000\n----------\nAccuracy:0.84612 | Loss:0.46303\nEpoch 10/2000\n----------\nAccuracy:0.84967 | Loss:0.46131\nEpoch 11/2000\n----------\nAccuracy:0.85105 | Loss:0.45948\nEpoch 12/2000\n----------\nAccuracy:0.85306 | Loss:0.45778\nEpoch 13/2000\n----------\nAccuracy:0.85502 | Loss:0.4562\nEpoch 14/2000\n----------\nAccuracy:0.85724 | Loss:0.45462\nEpoch 15/2000\n----------\nAccuracy:0.85937 | Loss:0.4531\nEpoch 16/2000\n----------\nAccuracy:0.86071 | Loss:0.45156\nEpoch 17/2000\n----------\nAccuracy:0.86255 | Loss:0.4505\nEpoch 18/2000\n----------\nAccuracy:0.86459 | Loss:0.44886\nEpoch 19/2000\n----------\nAccuracy:0.86576 | Loss:0.44735\nEpoch 20/2000\n----------\nAccuracy:0.86685 | Loss:0.44644\nEpoch 21/2000\n----------\nAccuracy:0.86881 | Loss:0.44501\nEpoch 22/2000\n----------\nAccuracy:0.8709 | Loss:0.44347\nEpoch 23/2000\n----------\nAccuracy:0.87203 | Loss:0.44245\nEpoch 24/2000\n----------\nAccuracy:0.87421 | Loss:0.44087\nEpoch 25/2000\n----------\nAccuracy:0.87504 | Loss:0.43977\nEpoch 26/2000\n----------\nAccuracy:0.87705 | Loss:0.43842\nEpoch 27/2000\n----------\nAccuracy:0.87918 | Loss:0.43702\nEpoch 28/2000\n----------\nAccuracy:0.87968 | Loss:0.43588\nEpoch 29/2000\n----------\nAccuracy:0.88098 | Loss:0.43485\nEpoch 30/2000\n----------\nAccuracy:0.88357 | Loss:0.43354\nEpoch 31/2000\n----------\nAccuracy:0.88499 | Loss:0.4324\nEpoch 32/2000\n----------\nAccuracy:0.88683 | Loss:0.4313\nEpoch 33/2000\n----------\nAccuracy:0.88833 | Loss:0.42982\nEpoch 34/2000\n----------\nAccuracy:0.88804 | Loss:0.42887\nEpoch 35/2000\n----------\nAccuracy:0.88967 | Loss:0.42739\nEpoch 36/2000\n----------\nAccuracy:0.89117 | Loss:0.42643\nEpoch 37/2000\n----------\nAccuracy:0.8931 | Loss:0.42475\nEpoch 38/2000\n----------\nAccuracy:0.89464 | Loss:0.42407\nEpoch 39/2000\n----------\nAccuracy:0.89623 | Loss:0.42259\nEpoch 40/2000\n----------\nAccuracy:0.89748 | Loss:0.42148\nEpoch 41/2000\n----------\nAccuracy:0.89807 | Loss:0.42043\nEpoch 42/2000\n----------\nAccuracy:0.89882 | Loss:0.41937\nEpoch 43/2000\n----------\nAccuracy:0.90028 | Loss:0.41821\nEpoch 44/2000\n----------\nAccuracy:0.9025 | Loss:0.41719\nEpoch 45/2000\n----------\nAccuracy:0.90342 | Loss:0.41596\nEpoch 46/2000\n----------\nAccuracy:0.90451 | Loss:0.41492\nEpoch 47/2000\n----------\nAccuracy:0.90597 | Loss:0.41386\nEpoch 48/2000\n----------\nAccuracy:0.90634 | Loss:0.41278\nEpoch 49/2000\n----------\nAccuracy:0.90802 | Loss:0.41168\nEpoch 50/2000\n----------\nAccuracy:0.90939 | Loss:0.41082\nEpoch 51/2000\n----------\nAccuracy:0.91006 | Loss:0.40981\nEpoch 52/2000\n----------\nAccuracy:0.91174 | Loss:0.4085\nEpoch 53/2000\n----------\nAccuracy:0.91236 | Loss:0.40757\nEpoch 54/2000\n----------\nAccuracy:0.91353 | Loss:0.40651\nEpoch 55/2000\n----------\nAccuracy:0.91483 | Loss:0.4056\nEpoch 56/2000\n----------\nAccuracy:0.91487 | Loss:0.40488\nEpoch 57/2000\n----------\nAccuracy:0.91658 | Loss:0.40368\nEpoch 58/2000\n----------\nAccuracy:0.91708 | Loss:0.40274\nEpoch 59/2000\n----------\nAccuracy:0.91809 | Loss:0.40185\nEpoch 60/2000\n----------\nAccuracy:0.91963 | Loss:0.40068\nEpoch 61/2000\n----------\nAccuracy:0.91988 | Loss:0.40023\nEpoch 62/2000\n----------\nAccuracy:0.92118 | Loss:0.39917\nEpoch 63/2000\n----------\nAccuracy:0.92223 | Loss:0.39797\nEpoch 64/2000\n----------\nAccuracy:0.92289 | Loss:0.39732\nEpoch 65/2000\n----------\nAccuracy:0.9234 | Loss:0.39649\nEpoch 66/2000\n----------\nAccuracy:0.92498 | Loss:0.39574\nEpoch 67/2000\n----------\nAccuracy:0.92565 | Loss:0.39487\nEpoch 68/2000\n----------\nAccuracy:0.92594 | Loss:0.39395\nEpoch 69/2000\n----------\nAccuracy:0.92762 | Loss:0.39305\nEpoch 70/2000\n----------\nAccuracy:0.9282 | Loss:0.39235\nEpoch 71/2000\n----------\nAccuracy:0.92808 | Loss:0.39198\nEpoch 72/2000\n----------\nAccuracy:0.92904 | Loss:0.39076\nEpoch 73/2000\n----------\nAccuracy:0.92983 | Loss:0.39012\nEpoch 74/2000\n----------\nAccuracy:0.93075 | Loss:0.38925\nEpoch 75/2000\n----------\nAccuracy:0.93213 | Loss:0.38813\nEpoch 76/2000\n----------\nAccuracy:0.93255 | Loss:0.38758\nEpoch 77/2000\n----------\nAccuracy:0.93284 | Loss:0.38698\nEpoch 78/2000\n----------\nAccuracy:0.93297 | Loss:0.38645\nEpoch 79/2000\n----------\nAccuracy:0.93372 | Loss:0.38588\nEpoch 80/2000\n----------\nAccuracy:0.93455 | Loss:0.38481\nEpoch 81/2000\n----------\nAccuracy:0.9351 | Loss:0.38429\nEpoch 82/2000\n----------\nAccuracy:0.93514 | Loss:0.38364\nEpoch 83/2000\n----------\nAccuracy:0.93585 | Loss:0.38305\nEpoch 84/2000\n----------\nAccuracy:0.93639 | Loss:0.3824\nEpoch 85/2000\n----------\nAccuracy:0.93669 | Loss:0.38188\nEpoch 86/2000\n----------\nAccuracy:0.93748 | Loss:0.38142\nEpoch 87/2000\n----------\nAccuracy:0.93794 | Loss:0.38059\nEpoch 88/2000\n----------\nAccuracy:0.9389 | Loss:0.37961\nEpoch 89/2000\n----------\nAccuracy:0.93923 | Loss:0.37924\nEpoch 90/2000\n----------\nAccuracy:0.93969 | Loss:0.37859\nEpoch 91/2000\n----------\nAccuracy:0.94015 | Loss:0.37821\nEpoch 92/2000\n----------\nAccuracy:0.94074 | Loss:0.37739\nEpoch 93/2000\n----------\nAccuracy:0.94162 | Loss:0.37696\nEpoch 94/2000\n----------\nAccuracy:0.94145 | Loss:0.37647\nEpoch 95/2000\n----------\nAccuracy:0.94241 | Loss:0.37576\nEpoch 96/2000\n----------\nAccuracy:0.94266 | Loss:0.37532\nEpoch 97/2000\n----------\nAccuracy:0.9432 | Loss:0.37459\nEpoch 98/2000\n----------\nAccuracy:0.94379 | Loss:0.3743\nEpoch 99/2000\n----------\nAccuracy:0.94433 | Loss:0.37377\nEpoch 100/2000\n----------\nAccuracy:0.94492 | Loss:0.37317\nEpoch 101/2000\n----------\nAccuracy:0.94509 | Loss:0.37285\nEpoch 102/2000\n----------\nAccuracy:0.94521 | Loss:0.3725\nEpoch 103/2000\n----------\nAccuracy:0.94559 | Loss:0.3722\nEpoch 104/2000\n----------\nAccuracy:0.94571 | Loss:0.37162\nEpoch 105/2000\n----------\nAccuracy:0.94617 | Loss:0.37116\nEpoch 106/2000\n----------\nAccuracy:0.94605 | Loss:0.37099\nEpoch 107/2000\n----------\nAccuracy:0.9463 | Loss:0.37062\nEpoch 108/2000\n----------\nAccuracy:0.94667 | Loss:0.37026\nEpoch 109/2000\n----------\nAccuracy:0.94697 | Loss:0.36966\nEpoch 110/2000\n----------\nAccuracy:0.94726 | Loss:0.36934\nEpoch 111/2000\n----------\nAccuracy:0.94726 | Loss:0.36923\nEpoch 112/2000\n----------\nAccuracy:0.94776 | Loss:0.3687\nEpoch 113/2000\n----------\nAccuracy:0.94784 | Loss:0.36847\nEpoch 114/2000\n----------\nAccuracy:0.94805 | Loss:0.36814\nEpoch 115/2000\n----------\nAccuracy:0.94876 | Loss:0.36758\nEpoch 116/2000\n----------\nAccuracy:0.94885 | Loss:0.36742\nEpoch 117/2000\n----------\nAccuracy:0.94918 | Loss:0.36698\nEpoch 118/2000\n----------\nAccuracy:0.94952 | Loss:0.3665\nEpoch 119/2000\n----------\nAccuracy:0.94956 | Loss:0.36618\nEpoch 120/2000\n----------\nAccuracy:0.94981 | Loss:0.36605\nEpoch 121/2000\n----------\nAccuracy:0.95039 | Loss:0.36545\nEpoch 122/2000\n----------\nAccuracy:0.95006 | Loss:0.36534\nEpoch 123/2000\n----------\nAccuracy:0.95073 | Loss:0.36491\nEpoch 124/2000\n----------\nAccuracy:0.95106 | Loss:0.36471\nEpoch 125/2000\n----------\nAccuracy:0.95115 | Loss:0.36446\nEpoch 126/2000\n----------\nAccuracy:0.95144 | Loss:0.36401\nEpoch 127/2000\n----------\nAccuracy:0.95148 | Loss:0.36413\nEpoch 128/2000\n----------\nAccuracy:0.95165 | Loss:0.36369\nEpoch 129/2000\n----------\nAccuracy:0.95169 | Loss:0.36355\nEpoch 130/2000\n----------\nAccuracy:0.95211 | Loss:0.36326\nEpoch 131/2000\n----------\nAccuracy:0.95215 | Loss:0.36307\nEpoch 132/2000\n----------\nAccuracy:0.95232 | Loss:0.36274\nEpoch 133/2000\n----------\nAccuracy:0.95223 | Loss:0.36268\nEpoch 134/2000\n----------\nAccuracy:0.95244 | Loss:0.36251\nEpoch 135/2000\n----------\nAccuracy:0.9524 | Loss:0.36236\nEpoch 136/2000\n----------\nAccuracy:0.95269 | Loss:0.36214\nEpoch 137/2000\n----------\nAccuracy:0.95261 | Loss:0.3623\nEpoch 138/2000\n----------\nAccuracy:0.9529 | Loss:0.36174\nEpoch 139/2000\n----------\nAccuracy:0.95294 | Loss:0.36173\nEpoch 140/2000\n----------\nAccuracy:0.95311 | Loss:0.36148\nEpoch 141/2000\n----------\nAccuracy:0.95294 | Loss:0.36141\nEpoch 142/2000\n----------\nAccuracy:0.95311 | Loss:0.36131\nEpoch 143/2000\n----------\nAccuracy:0.95315 | Loss:0.3612\nEpoch 144/2000\n----------\nAccuracy:0.95319 | Loss:0.36115\nEpoch 145/2000\n----------\nAccuracy:0.95361 | Loss:0.36096\nEpoch 146/2000\n----------\nAccuracy:0.95365 | Loss:0.36069\nEpoch 147/2000\n----------\nAccuracy:0.95374 | Loss:0.36056\nEpoch 148/2000\n----------\nAccuracy:0.9539 | Loss:0.36041\nEpoch 149/2000\n----------\nAccuracy:0.95395 | Loss:0.36034\nEpoch 150/2000\n----------\nAccuracy:0.95411 | Loss:0.36012\nEpoch 151/2000\n----------\nAccuracy:0.95411 | Loss:0.3601\nEpoch 152/2000\n----------\nAccuracy:0.95424 | Loss:0.35999\nEpoch 153/2000\n----------\nAccuracy:0.95411 | Loss:0.35997\nEpoch 154/2000\n----------\nAccuracy:0.95436 | Loss:0.35975\nEpoch 155/2000\n----------\nAccuracy:0.95457 | Loss:0.35947\nEpoch 156/2000\n----------\nAccuracy:0.9547 | Loss:0.35934\nEpoch 157/2000\n----------\nAccuracy:0.95466 | Loss:0.35936\nEpoch 158/2000\n----------\nAccuracy:0.95499 | Loss:0.35908\nEpoch 159/2000\n----------\nAccuracy:0.95516 | Loss:0.35886\nEpoch 160/2000\n----------\nAccuracy:0.95512 | Loss:0.35884\nEpoch 161/2000\n----------\nAccuracy:0.95528 | Loss:0.35872\nEpoch 162/2000\n----------\nAccuracy:0.95524 | Loss:0.35863\nEpoch 163/2000\n----------\nAccuracy:0.95541 | Loss:0.35843\nEpoch 164/2000\n----------\nAccuracy:0.95532 | Loss:0.35856\nEpoch 165/2000\n----------\nAccuracy:0.95545 | Loss:0.35837\nEpoch 166/2000\n----------\nAccuracy:0.95545 | Loss:0.35846\nEpoch 167/2000\n----------\nAccuracy:0.95541 | Loss:0.35874\nEpoch 168/2000\n----------\nAccuracy:0.95553 | Loss:0.35823\nEpoch 169/2000\n----------\nAccuracy:0.95578 | Loss:0.35799\nEpoch 170/2000\n----------\nAccuracy:0.95591 | Loss:0.35786\nEpoch 171/2000\n----------\nAccuracy:0.95595 | Loss:0.35772\nEpoch 172/2000\n----------\nAccuracy:0.95591 | Loss:0.35801\nEpoch 173/2000\n----------\nAccuracy:0.95591 | Loss:0.35792\nEpoch 174/2000\n----------\nAccuracy:0.95608 | Loss:0.35762\nEpoch 175/2000\n----------\nAccuracy:0.95608 | Loss:0.35752\nEpoch 176/2000\n----------\nAccuracy:0.95608 | Loss:0.35749\nEpoch 177/2000\n----------\nAccuracy:0.95616 | Loss:0.35743\nEpoch 178/2000\n----------\nAccuracy:0.9562 | Loss:0.35738\nEpoch 179/2000\n----------\nAccuracy:0.95612 | Loss:0.35765\nEpoch 180/2000\n----------\nAccuracy:0.95616 | Loss:0.35753\nEpoch 181/2000\n----------\nAccuracy:0.9562 | Loss:0.35757\nEpoch 182/2000\n----------\nAccuracy:0.95624 | Loss:0.35731\nEpoch 183/2000\n----------\nAccuracy:0.95637 | Loss:0.35715\nEpoch 184/2000\n----------\nAccuracy:0.95637 | Loss:0.35715\nEpoch 185/2000\n----------\nAccuracy:0.95637 | Loss:0.35715\nEpoch 186/2000\n----------\nAccuracy:0.95633 | Loss:0.35717\nEpoch 187/2000\n----------\nAccuracy:0.95637 | Loss:0.35717\nEpoch 188/2000\n----------\nAccuracy:0.95637 | Loss:0.35708\nEpoch 189/2000\n----------\nAccuracy:0.95641 | Loss:0.35712\nEpoch 190/2000\n----------\nAccuracy:0.95645 | Loss:0.35698\nEpoch 191/2000\n----------\nAccuracy:0.95608 | Loss:0.35772\nEpoch 192/2000\n----------\nAccuracy:0.95658 | Loss:0.35698\nEpoch 193/2000\n----------\nAccuracy:0.95658 | Loss:0.35686\nEpoch 194/2000\n----------\nAccuracy:0.95662 | Loss:0.35679\nEpoch 195/2000\n----------\nAccuracy:0.95666 | Loss:0.35679\nEpoch 196/2000\n----------\nAccuracy:0.95675 | Loss:0.35679\nEpoch 197/2000\n----------\nAccuracy:0.95679 | Loss:0.35667\nEpoch 198/2000\n----------\nAccuracy:0.95683 | Loss:0.35661\nEpoch 199/2000\n----------\nAccuracy:0.95687 | Loss:0.35662\nEpoch 200/2000\n----------\nAccuracy:0.95683 | Loss:0.35685\nEpoch 201/2000\n----------\nAccuracy:0.95679 | Loss:0.35678\nEpoch 202/2000\n----------\nAccuracy:0.95679 | Loss:0.35683\nEpoch 203/2000\n----------\nAccuracy:0.95683 | Loss:0.3566\nEpoch 204/2000\n----------\nAccuracy:0.95691 | Loss:0.35648\nEpoch 205/2000\n----------\nAccuracy:0.95683 | Loss:0.35662\nEpoch 206/2000\n----------\nAccuracy:0.957 | Loss:0.35646\nEpoch 207/2000\n----------\nAccuracy:0.957 | Loss:0.3564\nEpoch 208/2000\n----------\nAccuracy:0.95695 | Loss:0.35648\nEpoch 209/2000\n----------\nAccuracy:0.95704 | Loss:0.35636\nEpoch 210/2000\n----------\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24/372043131.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_train_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Accuracy:{round(float(train_acc),5)} | Loss:{round(float(train_loss),5)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_24/1456888855.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcorrect_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# iterate using a data loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Save dataset portions into variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model.fit(base_data, target_labels)\n\npreds = base_model.predict(base_data)\npreds","metadata":{"execution":{"iopub.status.busy":"2023-03-12T04:50:46.271538Z","iopub.status.idle":"2023-03-12T04:50:46.272019Z","shell.execute_reply.started":"2023-03-12T04:50:46.271774Z","shell.execute_reply":"2023-03-12T04:50:46.271798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\naccuracy = accuracy_score(target_labels, preds)\n\naccuracy","metadata":{"execution":{"iopub.status.busy":"2023-03-12T04:50:46.274195Z","iopub.status.idle":"2023-03-12T04:50:46.275335Z","shell.execute_reply.started":"2023-03-12T04:50:46.275045Z","shell.execute_reply":"2023-03-12T04:50:46.275071Z"},"trusted":true},"execution_count":null,"outputs":[]}]}