{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Part 1 - Data extraction and preprocessing\n","\n","Existing ARTK twitter dataset (widely used in sentiment analysis research) has been used for this project due to lack of access to API (and prohibitive costs)."]},{"cell_type":"markdown","metadata":{},"source":["# Import libraries"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T02:52:34.561185Z","iopub.status.busy":"2023-02-19T02:52:34.559938Z","iopub.status.idle":"2023-02-19T02:52:34.573412Z","shell.execute_reply":"2023-02-19T02:52:34.572237Z","shell.execute_reply.started":"2023-02-19T02:52:34.561133Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["SEED = 1234509876 # Set randomisation seed, used for consistency in results \n","\n","# Basic libraries\n","from zipfile import ZipFile\n","import os, sys\n","import re\n","import gc\n","import time\n","import datetime\n","\n","# Base Data processing and plotting\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Text processing specific libraries\n","import json \n","from string import punctuation\n","import emoji\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","# Data import-export\n","import pyarrow as pa\n","import pyarrow.parquet as pq\n","\n","# Others\n","from tqdm import tqdm_notebook #Loads progressbars for various loops\n","import warnings\n","warnings.filterwarnings('ignore')\n"]},{"cell_type":"markdown","metadata":{},"source":["## Useful settings and pre setup"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Useful matplotlib settings\n","%matplotlib inline\n","\n","# Useful pandas settings\n","pd.set_option('display.max_rows', 400)\n","pd.set_option('display.max_columns', 160)\n","pd.set_option('display.max_colwidth', 40)\n","warnings.filterwarnings(\"ignore\")\n","\n","# Download required text dictionaries\n","nltk.download('punkt')\n","nltk.download('stopwords')\n"]},{"cell_type":"markdown","metadata":{},"source":["# Custom Functions\n","\n","Collecting all functions here for easy reference and update"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T02:51:22.137341Z","iopub.status.busy":"2023-02-19T02:51:22.137052Z","iopub.status.idle":"2023-02-19T02:51:22.163442Z","shell.execute_reply":"2023-02-19T02:51:22.161876Z","shell.execute_reply.started":"2023-02-19T02:51:22.137312Z"},"trusted":true},"outputs":[],"source":["################################################################################################\n","# Downcasting function for pandas dataframes\n","\n","def downcast_dtypes(df):\n","    '''\n","    Changes column types in the dataframe:             \n","      `float64` type to lowest possible float without data loss\n","      `int64`   type to lowest possible int wihtout data loss\n","    '''\n","\n","    # Select columns to downcast\n","    float_cols = [col for col in df if df[col].dtype == \"float64\"]\n","    int_cols =   [col for col in df if df[col].dtype == \"int64\"]\n","\n","    # Downcast columns using to numeric function\n","    df[float_cols] = df[float_cols].apply(pd.to_numeric, downcast='float')\n","    df[int_cols] = df[int_cols].apply(pd.to_numeric, downcast='integer')\n","\n","    # remove variables from memory to avoid issues\n","\n","    del float_cols\n","    del int_cols\n","\n","    return df\n","\n","################################################################################################\n","# Check duplication at given level of dataframe\n","\n","def check_dups(df, cols):\n","\n","    orig_count_rows = df.shape[0]\n","\n","    temp = df.groupby(cols).size().reset_index(name = 'counts')\n","\n","    dedup_count_rows = temp.shape[0]\n","\n","    if orig_count_rows == dedup_count_rows:\n","        print(\"No duplicates. Dataframe is unique at given level\")\n","        print(\"# of unique entries: n=\",orig_count_rows)\n","    else:\n","        print(\"Duplicates found. Dataframe is not unique at given level\")\n","        print(\"# of entries in original dataset: n=\", orig_count_rows)\n","        print(\"# of unique entries expected in deduped dataset: n=\", dedup_count_rows)\n","        print(\"# of addational entries: n=\", orig_count_rows - dedup_count_rows)\n","\n","    del orig_count_rows, temp, dedup_count_rows\n","\n","#####################################################################################\n","# Plotting classification features\n","def fancy_plot(df):\n","    column_names = list(df.columns.values)\n","    frauds = df[df['Class'] == 1]\n","    no_frauds = df[df['Class'] == 0]\n","\n","    plt.figure()\n","    fig, ax = plt.subplots(8,4,figsize=(16,28))\n","    i = 0\n","    for feature in column_names:\n","        i += 1\n","        plt.subplot(8,4,i)\n","        sns.kdeplot(frauds[feature])\n","        sns.kdeplot(no_frauds[feature])\n","        plt.xlabel(feature, fontsize=10)\n","        locs, labels = plt.xticks()\n","        plt.tick_params(axis='both', which='major', labelsize=12)\n","    plt.show();\n","\n","####################################################################################\n","\n","########################################\n","#Custom function to apply functions to dataframe with missing values\n","def impute_missing(df, func, target_col, new_col_name):\n","    df.loc[~df[target_col].isnull(),new_col_name] = df.loc[~df[target_col].isnull(),target_col].apply(func)\n","\n","\n","####################################################################################\n","#text cleaning and stemming function. Modified to cater to text provided\n","\n","def remove_links(raw):\n","    # Extracts links from input text. Returns both text and links \n","    link_expr = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*(),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+.'\n","    \n","    # Check if passed object is single string or series\n","    if type(raw) == str:\n","        no_link_raw = re.sub(link_expr,\"\",raw)\n","        # links = re.findall(link_expr,\"\",raw)\n","    else:\n","        no_link_raw = list()\n","        # Disabled link extraction for now\n","        # links = list() \n","        \n","        for tweet in raw:\n","            no_link_raw.append(re.sub(link_expr, \"\", tweet))\n","            \n","    return no_link_raw\n","    \n","def remove_hashtags(raw):\n","    # Extracts links from input text. Returns both text and links\n","    # Will remove all trailing hashtags\n","    # Hashtags in middle of text will be replaced by a \"SOME_ENTITY\" constant text with hoope to retain context\n","    \n","    hashtag_expr = '#[A-Za-z0-9]+'\n","    middle_tag_expr = '#[A-Za-z0-9]+^[#]'\n","    # tags = re.findall(hashtag_expr,\"\",raw)\n","    \n","    if type(raw) == str:\n","        no_tag_raw = re.sub(hashtag_expr,\"\",raw)\n","        # links = re.findall(link_expr,\"\",raw)\n","    else:\n","        no_tag_raw = list()\n","        # Disabled link extraction for now\n","        # links = list() \n","        \n","        for tweet in raw:\n","            no_tag_raw.append(re.sub(hashtag_expr, \"\", tweet))\n","            \n","    return no_tag_raw\n","    \n","def replace_mentions(raw):\n","    # Replaces personal mentions with a common entity tag.\n","    # As we cannot build context on specific persons, we will tag it as entity and let our model identify language patterns\n","    mention_expr = '@[A-Za-z0-9]+'\n","    # tags = re.findall(hashtag_expr,\"\",raw)\n","    \n","    if type(raw) == str:\n","        no_mention_raw = re.sub(mention_expr,\" SOME_ENTITY \",raw) # Space to avoid potential merging with other words. \n","        # links = re.findall(link_expr,\"\",raw)\n","    else:\n","        no_tag_raw = list()\n","        # Disabled link extraction for now\n","        # links = list() \n","        \n","        for tweet in raw:\n","            no_tag_raw.append(re.sub(mention_expr,\" SOME_ENTITY \",tweet)) # Space to avoid potential merging with other words.\n","            \n","    return no_tag_raw\n","\n","def trim_extra_space(raw):\n","    space_expr = '\\s+'\n","    # tags = re.findall(hashtag_expr,\"\",raw)\n","    \n","    if type(raw) == str:\n","        clean_raw = re.sub(space_expr,\" \",raw)\n","        clean_raw = clean_raw.strip(\" \") # Remove end trails\n","        # links = re.findall(link_expr,\"\",raw)\n","    else:\n","        clean_raw = list()\n","        # Disabled link extraction for now\n","        # links = list() \n","        \n","        for tweet in raw:\n","            temp = re.sub(space_expr,\" \",tweet)\n","            clean_raw.append(temp.strip(\" \"))\n","            \n","    return clean_raw\n","\n","def clean_text(raw):\n","    # Combine all cleaning work\n","    cleaned_text = remove_links(raw)\n","    cleaned_text = remove_hashtags(cleaned_text)\n","    cleaned_text = replace_mentions(cleaned_text)\n","    cleaned_text = trim_extra_space(cleaned_text)    \n","\n","    return cleaned_text\n","\n","def simple_emoji_list(text):\n","    # Modification to emoji list function.\n","    # Removes the start and end character poiitns, and just retains actual emojis for easy parsing\n","    emojis = emoji.emoji_list(text)\n","    clean_list = list()\n","    if len(emojis)>0:\n","        for each in emojis:\n","            clean_list.append(emoji.demojize(each['emoji']))\n","    \n","    return clean_list\n","  \n","\n","# def token_converter():\n","    # Convert text to tokens\n","    \n","#     tokens = nltk.word_tokenize(temp)\n","    \n","#     alph_num_tokens = [word for word in tokens if word.isalnum()]\n","#     non_alph_num_tokens = [word for word in tokens if not word.isalnum()]\n","\n","#     non_alph_num_tokens = [word.split('-') for word in non_alph_num_tokens]\n","#     non_alph_num_tokens = nltk.flatten(non_alph_num_tokens)\n","#     non_alph_num_tokens = [word.split('.') for word in non_alph_num_tokens]\n","#     non_alph_num_tokens = nltk.flatten(non_alph_num_tokens)\n","\n","#     alph_num_tokens.extend(non_alph_num_tokens)\n","\n","#     tokens = nltk.flatten(alph_num_tokens)\n","\n","#     tokens = [porter.stem(word.lower()) for word in tokens]\n","#     tokens = [word for word in tokens if word not in stopwords_en]\n","#     tokens = [word for word in tokens if word.isalnum()]\n","\n","#     return tokens\n","\n","    #####################################################\n","# Generate word clouds\n","\n","def generate_wordclouds(X, in_X_tfidf, k, in_word_positions):\n","    # compute the total tfidf for each term in the cluster\n","    in_tfidf = in_X_tfidf[in_y_pred == in_cluster_id]\n","    # numpy.matrix\n","    tfidf_sum = np.sum(in_tfidf, axis=0)\n","    # numpy.array of shape (1, X.shape[1])\n","    tfidf_sum = np.asarray(tfidf_sum).reshape(-1)\n","    top_indices = tfidf_sum.argsort()[-top_count:]\n","    term_weights = {in_word_positions[in_idx]: tfidf_sum[in_idx] for in_idx in top_indices}\n","    wc = WordCloud(width=1200, height=800, background_color=\"white\")\n","    wordcloud = wc.generate_from_frequencies(term_weights)\n","    fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n","    ax.imshow(wordcloud, interpolation='bilinear')\n","    ax.axis(\"off\")\n","    fig.suptitle(f\"Cluster {in_cluster_id}\")\n","    plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Import dataset\n","\n","Dataset has been uploaded to kaggle repo for easy access"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:49:44.951826Z","iopub.status.idle":"2023-02-19T02:49:44.952212Z","shell.execute_reply":"2023-02-19T02:49:44.952073Z","shell.execute_reply.started":"2023-02-19T02:49:44.952058Z"},"trusted":true},"outputs":[],"source":["raw_data = pd.read_csv('/kaggle/input/sarcasm/all_twitter_sarcasam.csv')\n","\n","# Remove extra columns from the data\n","keep_cols = ['id','text']\n","raw_data = raw_data.loc[:,keep_cols]\n","\n","# Convert tweets to lowcase before proceeding with anything\n","raw_data['text'] = raw_data['text'].str.lower()"]},{"cell_type":"markdown","metadata":{},"source":["# Label data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:49:44.953723Z","iopub.status.idle":"2023-02-19T02:49:44.954140Z","shell.execute_reply":"2023-02-19T02:49:44.953950Z","shell.execute_reply.started":"2023-02-19T02:49:44.953930Z"},"trusted":true},"outputs":[],"source":["temp = raw_data.loc[:,'text']\n","labels = list()\n","sarcasm_tags = set((\"#sarcasm\",\"#sarcastic\"))\n","\n","for tweet in temp:\n","    lowcase_tweet = tweet.lower()\n","    hashtags = set(re.findall('#[A-Za-z0-9]+',lowcase_tweet))\n","    if any(hashtags & sarcasm_tags):\n","        labels.append(\"sarcasm\")\n","    else:\n","        labels.append(\"non-sarcasm\")\n","        \n","raw_data['labels'] = labels\n","raw_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Data Cleanup\n","\n","## Steps done:\n","1. Cleanup text:\n","    1. Lowercase text\n","    1. Remove hashtags\n","    1. Replace mentions with common tag \"SOME_ENTITY\"\n","    1. remove links (usually adverts have them)\n","1. Extract list of emojis for easy reference.\n","    1. Replace emojis with their text equivalents\n","    1. Remove the skin tone color information as that may not help with context\n","1. De-contract short form words (e.g. it's =>> it is)\n","1. TODO - Spell checking (not sure if it will help somehow.. Need to test later)\n","1. Lemmatization - Cannot use for BERT. Maybe we use normal word embeddings for first latyer, and BERT for 2nd..NOt sure"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:49:44.956019Z","iopub.status.idle":"2023-02-19T02:49:44.956479Z","shell.execute_reply":"2023-02-19T02:49:44.956270Z","shell.execute_reply.started":"2023-02-19T02:49:44.956230Z"},"trusted":true},"outputs":[],"source":["raw_data['clean_text'] = clean_text(raw_data['text'])\n","\n","# Remove duplicates. There are ton of ads that will be easy to remove after cleanup\n","# Ads contain different hashtags so cant be deduped raw\n","\n","raw_data.drop_duplicates(subset = 'clean_text', inplace = True)\n","raw_data = raw_data.reset_index().drop(columns = 'index')"]},{"cell_type":"markdown","metadata":{},"source":["## Extract emojis and replace with text\n","We will dedup and check if we need to standardize emojis and remove tone info"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:49:44.957925Z","iopub.status.idle":"2023-02-19T02:49:44.958359Z","shell.execute_reply":"2023-02-19T02:49:44.958144Z","shell.execute_reply.started":"2023-02-19T02:49:44.958123Z"},"trusted":true},"outputs":[],"source":["# Extract emojis for analysis\n","raw_data['emoji_list'] = raw_data['clean_text'].apply(simple_emoji_list)\n","# Replace emojis with text versions\n","raw_data['clean_text'] = raw_data['clean_text'].apply(emoji.demojize)\n","# Remove tone info to sandardize emojis\n","raw_data['clean_text'] = raw_data['clean_text'].apply(lambda text: re.sub('_[A-Za-z -]+_skin_tone:', \":\", text))\n","raw_data['emoji_list'] = raw_data['emoji_list'].apply(lambda text: [re.sub('_[A-Za-z -]+_skin_tone:', \":\", x) for x in text])\n","\n","raw_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Convert shortforms to longforms\n","\n","Solution is courtsey of https://stackoverflow.com/questions/43018030/replace-apostrophe-short-words-in-python"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:49:44.960760Z","iopub.status.idle":"2023-02-19T02:49:44.961175Z","shell.execute_reply":"2023-02-19T02:49:44.961021Z","shell.execute_reply.started":"2023-02-19T02:49:44.961004Z"},"trusted":true},"outputs":[],"source":["def decontracted(phrase):\n","    # Including all sorts of inverted commas to ensure text gets converted without issues with different posts using different keyboards\n","    \n","    # Specific exceptions to avoid mis spelled words\n","    phrase = re.sub(r\"won[\\'\\`\\â€™]t\", \"will not\", phrase)\n","    phrase = re.sub(r\"can[\\'\\`\\â€™]t\", \"can not\", phrase)\n","\n","    # general concatenations\n","    phrase = re.sub(r\"n[\\'\\`\\â€™]t\", \" not\", phrase)\n","    phrase = re.sub(r\"[\\'\\`\\â€™]re\", \" are\", phrase)\n","    phrase = re.sub(r\"[\\'\\`\\â€™]s\", \" is\", phrase)\n","    phrase = re.sub(r\"[\\'\\`\\â€™]d\", \" would\", phrase)\n","    phrase = re.sub(r\"[\\'\\`\\â€™]ll\", \" will\", phrase)\n","    phrase = re.sub(r\"[\\'\\`\\â€™]t\", \" not\", phrase)\n","    phrase = re.sub(r\"[\\'\\`\\â€™]ve\", \" have\", phrase)\n","    phrase = re.sub(r\"[\\'\\`\\â€™]m\", \" am\", phrase)\n","    return phrase\n","\n","print(raw_data.loc[11,'clean_text'])\n","raw_data['clean_text'] = raw_data['clean_text'].apply(decontracted)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Spell checking - RUNS SLOW. Expect 1-2 seconds per tweet. Ignore until final dataset\n","\n","Too time consuming. May need to run it once and store final data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:49:44.963241Z","iopub.status.idle":"2023-02-19T02:49:44.963781Z","shell.execute_reply":"2023-02-19T02:49:44.963619Z","shell.execute_reply.started":"2023-02-19T02:49:44.963602Z"},"trusted":true},"outputs":[],"source":["spell_checker = SpellChecker(distance=0) # TO reduce processing time and pick 1 closest word automatically.\n","\n","# Lambda function to return original word if spell check does not find anything\n","custom_spell_check = lambda word: word if spell_checker.correction(word) == None else spell_checker.correction(word)\n","\n","# raw_data['clean_text'] = raw_data['clean_text'].apply(lambda text: )\n","raw_data['clean_text'] = raw_data['clean_text'].map(lambda x:\" \".join(custom_spell_check(word) for word in x.split(\" \")))\n","\n","raw_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:49:44.964930Z","iopub.status.idle":"2023-02-19T02:49:44.965242Z","shell.execute_reply":"2023-02-19T02:49:44.965111Z","shell.execute_reply.started":"2023-02-19T02:49:44.965096Z"},"trusted":true},"outputs":[],"source":["# Saving the dataset as it takes too long now\n","import pyarrow as pa\n","import pyarrow.parquet as pq\n","\n","save_raw_data = pa.Table.from_pandas(raw_data)\n","pq.write_table(save_raw_data, '/kaggle/working/clean_raw_data.parquet')"]},{"cell_type":"markdown","metadata":{},"source":["## Lemmatization\n","\n","BERT should not like stopword removal and lemmatization due to how it works. \n","We will make a new column to store lemmatized + stopword removed text and see if performance changes"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T02:53:46.096098Z","iopub.status.busy":"2023-02-19T02:53:46.095740Z","iopub.status.idle":"2023-02-19T02:53:46.128928Z","shell.execute_reply":"2023-02-19T02:53:46.127946Z","shell.execute_reply.started":"2023-02-19T02:53:46.096073Z"},"trusted":true},"outputs":[],"source":["# # Import code for resume\n","# temp = pq.read_table('/kaggle/input/sarcasm/clean_raw_data.parquet')\n","\n","# raw_data = temp.to_pandas()"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T03:27:18.258826Z","iopub.status.busy":"2023-02-19T03:27:18.258508Z","iopub.status.idle":"2023-02-19T03:27:18.407308Z","shell.execute_reply":"2023-02-19T03:27:18.406120Z","shell.execute_reply.started":"2023-02-19T03:27:18.258803Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>labels</th>\n","      <th>clean_text</th>\n","      <th>emoji_list</th>\n","      <th>clean_text_lem</th>\n","      <th>clean_text_lem_stop</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1623471399825293312</td>\n","      <td>@annielayer @repmtg i'm sure the ame...</td>\n","      <td>sarcasm</td>\n","      <td>SOME_ENTITY SOME_ENTITY i am sure th...</td>\n","      <td>[]</td>\n","      <td>SOME_ENTITY SOME_ENTITY i am sure th...</td>\n","      <td>SOME_ENTITY SOME_ENTITY sure america...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1623470696125923329</td>\n","      <td>get my art printed on awesome produc...</td>\n","      <td>sarcasm</td>\n","      <td>get my art printed on awesome produc...</td>\n","      <td>[]</td>\n","      <td>get my art printed on awesome produc...</td>\n","      <td>get art printed awesome product redo...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1623467236982947842</td>\n","      <td>trudeau? anyone? #tuckercarlson #unh...</td>\n","      <td>sarcasm</td>\n","      <td>trudeau? anyone</td>\n","      <td>[]</td>\n","      <td>trudeau? anyone</td>\n","      <td>trudeau? anyone</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1623465163792711681</td>\n","      <td>nuh uh. #joebiden told me everything...</td>\n","      <td>sarcasm</td>\n","      <td>nuh uh told me everything was fine.:...</td>\n","      <td>[:woman_shrugging:]</td>\n","      <td>nuh uh told me everything wa fine.:w...</td>\n","      <td>nuh uh told everything wa fine.:woma...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1623465117869395968</td>\n","      <td>ðŸ˜‚ she gave #biden the #chinaballoon ...</td>\n","      <td>sarcasm</td>\n","      <td>:face_with_tears_of_joy: she gave th...</td>\n","      <td>[:face_with_tears_of_joy:, :rolling_...</td>\n","      <td>:face_with_tears_of_joy: she gave th...</td>\n","      <td>:face_with_tears_of_joy: gave :rolli...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                    id                                     text   labels  \\\n","0  1623471399825293312  @annielayer @repmtg i'm sure the ame...  sarcasm   \n","1  1623470696125923329  get my art printed on awesome produc...  sarcasm   \n","2  1623467236982947842  trudeau? anyone? #tuckercarlson #unh...  sarcasm   \n","3  1623465163792711681  nuh uh. #joebiden told me everything...  sarcasm   \n","4  1623465117869395968  ðŸ˜‚ she gave #biden the #chinaballoon ...  sarcasm   \n","\n","                                clean_text  \\\n","0  SOME_ENTITY SOME_ENTITY i am sure th...   \n","1  get my art printed on awesome produc...   \n","2                          trudeau? anyone   \n","3  nuh uh told me everything was fine.:...   \n","4  :face_with_tears_of_joy: she gave th...   \n","\n","                                emoji_list  \\\n","0                                       []   \n","1                                       []   \n","2                                       []   \n","3                      [:woman_shrugging:]   \n","4  [:face_with_tears_of_joy:, :rolling_...   \n","\n","                            clean_text_lem  \\\n","0  SOME_ENTITY SOME_ENTITY i am sure th...   \n","1  get my art printed on awesome produc...   \n","2                          trudeau? anyone   \n","3  nuh uh told me everything wa fine.:w...   \n","4  :face_with_tears_of_joy: she gave th...   \n","\n","                       clean_text_lem_stop  \n","0  SOME_ENTITY SOME_ENTITY sure america...  \n","1  get art printed awesome product redo...  \n","2                          trudeau? anyone  \n","3  nuh uh told everything wa fine.:woma...  \n","4  :face_with_tears_of_joy: gave :rolli...  "]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["temp = raw_data.loc[:,['clean_text']]\n","\n","lemmatizer = WordNetLemmatizer()\n","stopword_list = stopwords.words('english')\n","\n","# Can possibly integrate POS here it seems\n","raw_data['clean_text_lem'] = raw_data['clean_text'].apply(lambda tweet: \" \".join(lemmatizer.lemmatize(word) for word in tweet.split(\" \")))\n","# Remove stopwords\n","raw_data['clean_text_lem_stop'] = raw_data['clean_text_lem'].apply(lambda tweet: \" \".join(word for word in tweet.split(\" \") if word not in stopword_list))\n","\n","raw_data.head()"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-02-19T03:28:06.001874Z","iopub.status.busy":"2023-02-19T03:28:06.001515Z","iopub.status.idle":"2023-02-19T03:28:06.023235Z","shell.execute_reply":"2023-02-19T03:28:06.022558Z","shell.execute_reply.started":"2023-02-19T03:28:06.001850Z"},"trusted":true},"outputs":[],"source":["# Save output for easy use::\n","\n","\n","save_raw_data = pa.Table.from_pandas(raw_data)\n","pq.write_table(save_raw_data, '/kaggle/working/clean_raw_data.parquet')"]},{"cell_type":"markdown","metadata":{},"source":["# Quick EDA\n","\n","Check simple word density based on the labelled tweets"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:49:44.970379Z","iopub.status.idle":"2023-02-19T02:49:44.970672Z","shell.execute_reply":"2023-02-19T02:49:44.970543Z","shell.execute_reply.started":"2023-02-19T02:49:44.970526Z"},"trusted":true},"outputs":[],"source":["sarcasm_tweets = raw_data.loc[raw_data['labels'] == 'sarcasm','clean_text']\n","sarcasm_all_words = ' '.join(sarcasm_tweets)\n","\n","from wordcloud import WordCloud\n","\n","wordcloud = WordCloud(width=800, height=800, background_color='white', max_words=100, colormap='viridis', contour_width=3, contour_color='black')\n","\n","# Generate the word cloud\n","wordcloud.generate(sarcasm_all_words)\n","plt.figure(figsize=(8,8), facecolor=None)\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.tight_layout(pad=0)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:49:44.971825Z","iopub.status.idle":"2023-02-19T02:49:44.972125Z","shell.execute_reply":"2023-02-19T02:49:44.971988Z","shell.execute_reply.started":"2023-02-19T02:49:44.971974Z"},"trusted":true},"outputs":[],"source":["# Simple table with frequency to check if there has been issue with prepriocessing\n","tqdm_notebook.pandas(desc = \"Spell check progress\")\n","all_word_list = pd.DataFrame(data = (\" \".join(raw_data['clean_text']).split(\" \")), columns = [\"words\"])\n","all_word_list['count'] = 1\n","\n","word_count = all_word_list.groupby(['words'],as_index = False).count()\n","word_count.sort_values(by = 'count', ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Feature Generation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:49:44.973123Z","iopub.status.idle":"2023-02-19T02:49:44.973467Z","shell.execute_reply":"2023-02-19T02:49:44.973342Z","shell.execute_reply.started":"2023-02-19T02:49:44.973328Z"},"trusted":true},"outputs":[],"source":["plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Notes::\n","\n","1. Maybe number of hashtags as feature. "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
